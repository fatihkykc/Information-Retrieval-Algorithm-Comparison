{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PrettyTable in /home/fatihkykc/.local/lib/python3.6/site-packages (0.7.2)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install PrettyTable --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MEfgPOu5grRo"
   },
   "outputs": [],
   "source": [
    "import io, os\n",
    "import re as re\n",
    "import zipfile as zipfile\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mN8iG5JYgzdp"
   },
   "outputs": [],
   "source": [
    "textsWithTextNumbers = {}       # { doc name(number) : text }\n",
    "\n",
    "mytextzip = ''\n",
    "path = './30Columnists.zip'\n",
    "\n",
    "with zipfile.ZipFile(path) as z:\n",
    "    for zipinfo in z.infolist():\n",
    "        if zipinfo.filename.endswith('.txt') and re.search('raw_texts', zipinfo.filename):\n",
    "            with z.open(zipinfo) as f:\n",
    "                textfile = io.TextIOWrapper(f, encoding='cp1254', newline='')\n",
    "                for line in textfile:\n",
    "                    if len(line.strip()): mytextzip += line.strip()\n",
    "\n",
    "                filename = str(zipinfo.filename)\n",
    "                filename = filename.split(\"/\")\n",
    "                filename = filename[-1][:-4]\n",
    "                \n",
    "                textsWithTextNumbers[filename] = mytextzip\n",
    "                mytextzip = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vEn_AC6rgtH3"
   },
   "outputs": [],
   "source": [
    "stopwords = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0U4cqN9JtKNN"
   },
   "outputs": [],
   "source": [
    "def Tokenizer(text, stop):\n",
    "    text = (str(text)).lower()          # normalize\n",
    "    words = re.findall(r'\\w+', text)    # find all words in it\n",
    "\n",
    "    if stop == 1:\n",
    "        #remove stopwords, start last word to avoid index problems in loop\n",
    "        for x in range(len(words)-1, -1, -1):\n",
    "            word = str(words[x])\n",
    "            if word in stopwords:\n",
    "                words.pop(x)\n",
    "            elif word.lower() in stopwords:\n",
    "                words.pop(x)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qqny0F0rg2qT"
   },
   "outputs": [],
   "source": [
    "docsWords = {}      # { doc name(number) : words in it }\n",
    "\n",
    "def turner(textsWithTextNumbers):\n",
    "    temp = {}\n",
    "    for doc, text in textsWithTextNumbers.items():\n",
    "        temp[doc] = Tokenizer(text, 1)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9Cvjd1byV8i"
   },
   "outputs": [],
   "source": [
    "docsWords = turner(textsWithTextNumbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNgbqjLqcfQp"
   },
   "outputs": [],
   "source": [
    "# database to keep words with docs where they are in\n",
    "db = {}    # \"42\" : ['1.1', '1.32', ...]\n",
    "db_ = {}     # \"42\" : [('1.1', 1),...}\n",
    "\n",
    "for doc, words in docsWords.items():\n",
    "    words = sorted(words)\n",
    "    n = 1\n",
    "    for i, word in enumerate(words):\n",
    "        if (i+1) != len(words):\n",
    "            nextw = words[i+1]\n",
    "            if word == nextw:\n",
    "                n += 1\n",
    "                continue\n",
    "        if word not in db:\n",
    "            db[word] = []\n",
    "            db_[word] = []\n",
    "        db[word].append(doc)\n",
    "        db_[word].append((doc, n))\n",
    "\n",
    "        n = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Og3SWgSyIZUH"
   },
   "outputs": [],
   "source": [
    "term_frequencies = {}       # { doc: {word: word frequency} }\n",
    "                            # e.g. '1.1': [['doubt', 0.003745318352059925],...]\n",
    "\n",
    "for doc, words in docsWords.items():\n",
    "    d = {}\n",
    "    temp = []\n",
    "    \n",
    "    #term frequency\n",
    "    for word in words:\n",
    "        if word not in d:\n",
    "            d[word] = 1\n",
    "        else:\n",
    "            d[word] += 1\n",
    "\n",
    "    n = len(d)      # of words in doc\n",
    "    for word, number in d.items():\n",
    "        tf = number / n\n",
    "        temp.append([word, tf])\n",
    "\n",
    "    term_frequencies[doc] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0q0hy5YmacS"
   },
   "outputs": [],
   "source": [
    "idfs = {}            # {word: number} ---> {word: idf}\n",
    "                     # e.g. {'doubt': 3.1958930192167068}\n",
    "\n",
    "for doc, array in term_frequencies.items():\n",
    "    for word, tf in array:\n",
    "        if word not in idfs:\n",
    "            idfs[word] = 1\n",
    "        else:\n",
    "            idfs[word] += 1\n",
    "\n",
    "doc_total = len(term_frequencies)\n",
    "for word, n in idfs.items():\n",
    "    idf = ((1 + doc_total) / (n+1))\n",
    "    idfs[word] = math.log(idf) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGTd9K__06BZ"
   },
   "outputs": [],
   "source": [
    "# TF-IDF = TF * IDF\n",
    "\n",
    "tfidf = {}\n",
    "#{doc: {word: tf-idf}}\n",
    "# e.g. '1.11': {'time': 0.009378893551214249, 'year': 0.02088227471955536,...} \n",
    "# e.g. '1.12': {'time': 0.010133140235402807, 'year': 0.011280809245354205,...}\n",
    "\n",
    "for doc, array in term_frequencies.items():\n",
    "    tfidf[doc] = {}\n",
    "    for word, tf in array:\n",
    "        TfIdf = tf * idfs[word]\n",
    "        tfidf[doc][word] = TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(words) for words in term_frequencies.values()]\n",
    "avgN = sum(lens) / len(lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okapi BM25\n",
    "BM25 Algorithm is  \n",
    "sum of ((IDF for word in query) times  \n",
    "((frequency of the word in document) times (@paramK+1)) over  \n",
    "((frequency of the word in document + @paramK) times ((1 - @paramB) + (b times (length of document over avg length of document)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_BM25(IDF, TF, K, B, N, avgN):\n",
    "    first = TF * (K + 1)\n",
    "    second = TF + (K * ((1 - B) + (B * (N / avgN))))\n",
    "    return IDF * (first / second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IDF = idfs['doubt']\n",
    "# TF = term_frequencies['1.1'][0][1]\n",
    "\n",
    "# N = len(term_frequencies['1.1'])\n",
    "# lens = [len(words) for words in term_frequencies.values()]\n",
    "# avgN = sum(lens) / len(lens)\n",
    "\n",
    "# calc_BM25(IDF, TF, K, B, N, avgN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "BM = defaultdict(lambda: defaultdict(int))\n",
    "#{doc: {word: bm}}\n",
    "lens = [len(words) for words in term_frequencies.values()]\n",
    "avgN = sum(lens) / len(lens)\n",
    "K = 1.6\n",
    "B = 0.75\n",
    "for doc in term_frequencies:\n",
    "    N = len(term_frequencies[doc])\n",
    "    for idx, word in enumerate(term_frequencies[doc]):\n",
    "        TF = word[1]\n",
    "        IDF = idfs[word[0]]\n",
    "        BM[doc][word[0]] = calc_BM25(IDF, TF, K, B, N, avgN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFI\n",
    "e = frequency of the word in corpus times length of the document over number of documents\n",
    "\n",
    "DFI = log((frequency of the word in the document - e) over e squared) on base 2\n",
    "\n",
    "TF = total frequency of the word in corpus  \n",
    "\n",
    "tf = frequency of the word in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calc_DFI(TF,tf, n, N):\n",
    "    e = (TF * n) / N\n",
    "    return log( ( ( tf - e ) / math.sqrt(e) ) + 1 )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "DFI = defaultdict(lambda: defaultdict(int))\n",
    "TFDict = defaultdict()\n",
    "N = sum([len(term_frequencies[doc]) for doc in term_frequencies])\n",
    "\n",
    "\n",
    "for doc in term_frequencies:\n",
    "    for termTF in term_frequencies[doc]:\n",
    "        if termTF[0] in TFDict:\n",
    "            TFDict[termTF[0]] += termTF[1]\n",
    "        else:\n",
    "            TFDict[termTF[0]] = termTF[1]\n",
    "\n",
    "            \n",
    "for doc in term_frequencies:\n",
    "    n = len(term_frequencies[doc])\n",
    "    for termTF in term_frequencies[doc]:\n",
    "        tf = termTF[1]\n",
    "        TF = TFDict[termTF[0]]\n",
    "        DFI[doc][termTF[0]] = calc_DFI(TF, tf, n, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AW18P2_za2mn"
   },
   "outputs": [],
   "source": [
    "def clusterDB(entry):\n",
    "    NinDB = []      # words in entry but not in DB\n",
    "    inDB = []       # words in entry and DB\n",
    "\n",
    "    # cluster words in entry\n",
    "    for word in entry:\n",
    "        word = word.lower()\n",
    "        if word not in db and word not in NinDB:\n",
    "            NinDB.append(word)\n",
    "        elif word in db and word not in inDB:\n",
    "            inDB.append(word)\n",
    "    \n",
    "    return inDB, NinDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dh4g5Halcko6"
   },
   "outputs": [],
   "source": [
    "def findDualTuples(inDB):\n",
    "    docs = {}       # words tuple: common documents where they are              # ('gold', 'rich'): ['26.12', '15.50']\n",
    "    w = []          # words tuples                                              # [('gold', 'rich'), ..., ('gold', 'man', 'rich')]\n",
    "\n",
    "    temp = inDB.copy()     # for inner loop -> remove words itself every main loop\n",
    "\n",
    "    # compare words documents where they are to find common docs\n",
    "    # just for find dual tuples\n",
    "    for word1 in inDB:\n",
    "        temp.remove(word1)\n",
    "        for word2 in temp:\n",
    "            join = list(set(db[word1]) & set(db[word2]))\n",
    "            if len(join) > 0:\n",
    "                w.append((word1, word2))\n",
    "                docs[(word1, word2)] = join\n",
    "\n",
    "    return docs, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6PJggk8ldT4X"
   },
   "outputs": [],
   "source": [
    "def findAllIntersections(docs, w):\n",
    "    # detect common docs between every words\n",
    "    # finds all sub and main intersections\n",
    "    for i, tuple1 in enumerate(w):\n",
    "        \n",
    "        # there is no second tuple to find commons\n",
    "        if (len(w)-1) == i:\n",
    "            break\n",
    "\n",
    "        # for inner loop -> don't need to look itself and before tuples\n",
    "        inner = w[(i+1):]\n",
    "\n",
    "        # tuples that have same length can be compared because of hierarchy\n",
    "        l1 = len(tuple1)\n",
    "        for tuple2 in inner:\n",
    "            l2 = len(tuple2)\n",
    "            if l1 != l2:\n",
    "                continue\n",
    "\n",
    "            # tuples(words) are combined\n",
    "            n = tuple1 + tuple2\n",
    "            # to avoid being listed separate classification of different rankings of the same words\n",
    "            n = sorted(n)\n",
    "            # to avoid using the same word more than once\n",
    "            n = tuple(dict.fromkeys(n))\n",
    "            # find common docs\n",
    "            join = list(set(docs[tuple1]) & set(docs[tuple2]))\n",
    "\n",
    "            # there is at least one common document and group of same words were not found before by different tuple pairs\n",
    "            if len(join) > 0 and n not in w:\n",
    "                w.append(n)\n",
    "                docs[n] = join\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7J90PC0e-Da"
   },
   "outputs": [],
   "source": [
    "def fitResults(docs, mod):\n",
    "    # keep related keys, docs and tf-idfs in same index\n",
    "    x = []      # [doc]\n",
    "    y = []      # [keys]\n",
    "    z = []      # [total tfidfs]\n",
    "\n",
    "    for tuplex, docs in docs.items():      # tuplex -> word tuples, docs -> keys common docs\n",
    "        for doc in docs:\n",
    "            t = 0\n",
    "            for word in tuplex:\n",
    "                if mod == \"tfidf\":\n",
    "                    t += tfidf[doc][word]   # word's tf-idf value in doc\n",
    "                                            # sum of values represent relation between tuple(words) and doc\n",
    "                elif mod == \"bm25\":\n",
    "                    t+= BM[doc][word]\n",
    "                else:\n",
    "                    t+= DFI[doc][word]\n",
    "                \n",
    "            \n",
    "            # to check whether the value has been calculated beforehand for this doc\n",
    "            if doc in x:\n",
    "                i = x.index(doc)\n",
    "                if z[i] > t:     # this doc was associated before with better tuple\n",
    "                    continue\n",
    "                else:            # this tuple better, delete old one\n",
    "                    x.pop(i)\n",
    "                    y.pop(i)\n",
    "                    z.pop(i)\n",
    "            # add same index\n",
    "            x.append(doc)       # doc\n",
    "            y.append(tuplex)    # tuple\n",
    "            z.append(t)         # sum of tf-idf values of words for this doc\n",
    "\n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmk90pGqeUIw"
   },
   "outputs": [],
   "source": [
    "# number of terms important\n",
    "def sortResults(x, y, z):\n",
    "    r = []\n",
    "    for i in range(len(x)):\n",
    "        # take max relavent index\n",
    "        maxIdx = np.argmax(z)\n",
    "        \n",
    "        # add result to results\n",
    "        a = [x[maxIdx], str(y[maxIdx]), z[maxIdx], len(y[maxIdx])]\n",
    "        \n",
    "        # delete until there is no result\n",
    "        x.pop(maxIdx)\n",
    "        y.pop(maxIdx)\n",
    "        z.pop(maxIdx)\n",
    "\n",
    "        r.append(a)\n",
    "\n",
    "    temp = {}\n",
    "    for xx, yy, zz, ll in r:\n",
    "        if ll not in temp.keys():\n",
    "            temp[ll] = []\n",
    "        temp[ll].append([xx, yy, zz])\n",
    "\n",
    "    temp_items = temp.items()\n",
    "    sorted_items = sorted(temp_items, reverse= True)\n",
    "\n",
    "    results = []\n",
    "    for key, part in sorted_items:\n",
    "        for a in part:\n",
    "            results.append(a)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q07Ia-mFSDju"
   },
   "outputs": [],
   "source": [
    "# 1-1\n",
    "def OneByOne(wordsInDB, mod):\n",
    "    x = []  # docs\n",
    "    y = []  # terms\n",
    "    z = []  # tfidfs\n",
    "    \n",
    "    for term in wordsInDB:\n",
    "        docus = db[term]\n",
    "        for doc in docus:\n",
    "            if mod == \"tfidf\":\n",
    "                v = tfidf[doc][term]\n",
    "            elif mod == \"bm25\":\n",
    "                v = BM[doc][term]\n",
    "            else:\n",
    "                v = DFI[doc][term]\n",
    "            \n",
    "            x.append(doc)\n",
    "            y.append(term)\n",
    "            z.append(v)\n",
    "\n",
    "    r = []\n",
    "    for i in range(len(x)):\n",
    "        # take max relavent index\n",
    "        maxIdx = np.argmax(z)\n",
    "        # add result to results\n",
    "        a = [x[maxIdx], str(y[maxIdx]), z[maxIdx]]\n",
    "        # delete until there is no result\n",
    "        x.pop(maxIdx)\n",
    "        y.pop(maxIdx)\n",
    "        z.pop(maxIdx)\n",
    "\n",
    "        r.append(a)\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6fiwptoSobi"
   },
   "outputs": [],
   "source": [
    "def Search(entry, mod):\n",
    "    entry = Tokenizer(entry, 0)\n",
    "\n",
    "    # cluster words in entry\n",
    "    NinDB = []      # words in entry but not in DB\n",
    "    inDB = []       # words in entry and DB\n",
    "    inDB, NinDB = clusterDB(entry)\n",
    "\n",
    "    # warning message for words that not in DB\n",
    "    if len(inDB) == 0:\n",
    "        print(\"\\nThere is no doc that include these terms. Try again.\")\n",
    "        return\n",
    "    if len(NinDB) != 0:\n",
    "        p = \" - \"\n",
    "        warning = p.join(NinDB)\n",
    "        print(\"\\n'\" + warning + \"' -> not in DB\")\n",
    "\n",
    "    # there is just one word\n",
    "    if len(inDB) == 1:\n",
    "        return OneByOne(inDB, mod)\n",
    "\n",
    "    elif len(inDB) > 1:\n",
    "        # find dual tuples\n",
    "        docs = {}       # words tuple: common documents where they are              # ('gold', 'rich'): ['26.12', '15.50']\n",
    "        w = []          # words tuples                                              # [('gold', 'rich'), ..., ('gold', 'man', 'rich')]    \n",
    "        docs, w = findDualTuples(inDB)\n",
    "        \n",
    "        # no common docs between every dual words documents\n",
    "        if len(w) == 0:\n",
    "            print(\"\\nNo common documents!\")\n",
    "            return OneByOne(inDB, mod)\n",
    "\n",
    "        # detect common docs between every words\n",
    "        # finds all sub and main intersections\n",
    "        docs = findAllIntersections(docs, w)\n",
    "\n",
    "        # keep related keys, docs and tf-idfs in same index\n",
    "        x, y, z = fitResults(docs, mod)\n",
    "\n",
    "        return sortResults(x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GB8uorWejL_o"
   },
   "outputs": [],
   "source": [
    "def printResults(r):\n",
    "    print(\"\\n\"+str(len(r)) + \" search results were found.\" )\n",
    "    t = PrettyTable([\"Doc\", \"Terms\", \"Relevance\"])\n",
    "\n",
    "    for i, result in enumerate(r):\n",
    "        t.add_row(result)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11609,
     "status": "ok",
     "timestamp": 1589251717793,
     "user": {
      "displayName": "WhineyB",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj_Vi2WUiksXxY70zz29ccNDzRabkMQHNJW4rgF0A=s64",
      "userId": "14308996959334696623"
     },
     "user_tz": -180
    },
    "id": "Kn_LToEVqwvD",
    "outputId": "ac580c58-2b64-48ae-ed0d-d32e0bbddf3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select your preffered model: tfidf / bm25 / dfidfi\n",
      "dfi\n",
      "Enter a query: breast cancer risk for women\n",
      "\n",
      "'for' -> not in DB\n",
      "\n",
      "123 search results were found.\n",
      "+-------+---------------------------------------+-----------------------+\n",
      "|  Doc  |                 Terms                 |       Relevance       |\n",
      "+-------+---------------------------------------+-----------------------+\n",
      "| 24.48 | ('breast', 'cancer', 'risk', 'women') |   5.456583863614265   |\n",
      "| 24.49 | ('breast', 'cancer', 'risk', 'women') |   5.456583863614265   |\n",
      "| 24.50 | ('breast', 'cancer', 'risk', 'women') |   5.456583863614265   |\n",
      "| 24.23 | ('breast', 'cancer', 'risk', 'women') |    5.04164142544976   |\n",
      "|  24.7 | ('breast', 'cancer', 'risk', 'women') |   4.044986118636013   |\n",
      "|  9.37 | ('breast', 'cancer', 'risk', 'women') |   3.9894709400103903  |\n",
      "| 24.47 | ('breast', 'cancer', 'risk', 'women') |    3.97902997972275   |\n",
      "| 11.35 | ('breast', 'cancer', 'risk', 'women') |   3.8764694279172325  |\n",
      "|  24.2 | ('breast', 'cancer', 'risk', 'women') |   3.273363033671877   |\n",
      "|  24.3 | ('breast', 'cancer', 'risk', 'women') |   3.239869606856905   |\n",
      "|  24.6 | ('breast', 'cancer', 'risk', 'women') |   3.019757138438166   |\n",
      "| 24.34 | ('breast', 'cancer', 'risk', 'women') |   2.481621628493243   |\n",
      "|  9.1  | ('breast', 'cancer', 'risk', 'women') |   2.136843829014067   |\n",
      "| 11.18 | ('breast', 'cancer', 'risk', 'women') |   2.133980622222496   |\n",
      "| 16.48 | ('breast', 'cancer', 'risk', 'women') |   1.9703188260815672  |\n",
      "| 27.16 | ('breast', 'cancer', 'risk', 'women') |   1.7589410042720823  |\n",
      "| 24.39 | ('breast', 'cancer', 'risk', 'women') |   1.3852817615563093  |\n",
      "|  11.9 | ('breast', 'cancer', 'risk', 'women') |   0.9922265658891424  |\n",
      "| 11.31 | ('breast', 'cancer', 'risk', 'women') |   0.949146874274704   |\n",
      "| 27.41 |     ('breast', 'cancer', 'women')     |   3.337814328488773   |\n",
      "|  9.39 |     ('breast', 'cancer', 'women')     |   3.066391971844337   |\n",
      "|  9.40 |     ('breast', 'cancer', 'women')     |   2.750060831952215   |\n",
      "| 27.35 |      ('cancer', 'risk', 'women')      |   2.5553006759709596  |\n",
      "| 11.20 |      ('cancer', 'risk', 'women')      |   2.1119758965181314  |\n",
      "| 24.46 |     ('breast', 'cancer', 'women')     |   2.0262897940504048  |\n",
      "|  9.43 |     ('breast', 'cancer', 'women')     |   1.957951323864812   |\n",
      "| 24.33 |     ('breast', 'cancer', 'women')     |   1.928733768990086   |\n",
      "|  9.29 |      ('cancer', 'risk', 'women')      |   1.6530726814694785  |\n",
      "| 24.12 |      ('cancer', 'risk', 'women')      |   1.6478325684079393  |\n",
      "|  24.1 |     ('breast', 'cancer', 'women')     |   1.475176190063052   |\n",
      "| 25.14 |     ('breast', 'cancer', 'women')     |    1.34180371755602   |\n",
      "| 16.49 |      ('cancer', 'risk', 'women')      |   1.2943275421544114  |\n",
      "|  9.41 |      ('breast', 'cancer', 'risk')     |   1.168275725500528   |\n",
      "| 29.40 |      ('cancer', 'risk', 'women')      |   1.1059115353901257  |\n",
      "| 27.26 |      ('cancer', 'risk', 'women')      |   1.0112653356719923  |\n",
      "| 11.15 |     ('breast', 'cancer', 'women')     |   0.9654094803722083  |\n",
      "| 24.19 |      ('breast', 'cancer', 'risk')     |   0.6011090046735809  |\n",
      "|  24.5 |      ('breast', 'cancer', 'risk')     |   0.5247607700068437  |\n",
      "| 16.13 |      ('breast', 'risk', 'women')      |   0.471500007391055   |\n",
      "| 16.22 |     ('breast', 'cancer', 'women')     |   0.4702288741806505  |\n",
      "| 24.25 |     ('breast', 'cancer', 'women')     |   0.382219743419626   |\n",
      "|  9.33 |      ('cancer', 'risk', 'women')      |   0.3140102210898768  |\n",
      "|  16.4 |      ('cancer', 'risk', 'women')      |  0.25170604173078104  |\n",
      "| 16.10 |      ('cancer', 'risk', 'women')      |  0.05059215139737107  |\n",
      "|  27.6 |          ('breast', 'cancer')         |   4.704425983006865   |\n",
      "| 27.43 |           ('risk', 'women')           |    2.71203810950748   |\n",
      "|  11.8 |           ('cancer', 'risk')          |   2.2269060341685756  |\n",
      "| 27.19 |           ('cancer', 'risk')          |   2.153602326950211   |\n",
      "| 11.49 |           ('risk', 'women')           |   1.9531137148176123  |\n",
      "| 24.32 |          ('breast', 'cancer')         |   1.833508977017291   |\n",
      "| 11.27 |           ('cancer', 'risk')          |   1.798038097006644   |\n",
      "| 11.34 |           ('risk', 'women')           |   1.6828747633723062  |\n",
      "| 24.16 |          ('breast', 'cancer')         |   1.6424400631122298  |\n",
      "| 24.42 |          ('cancer', 'women')          |   1.558476203415379   |\n",
      "| 11.41 |           ('risk', 'women')           |   1.4294210480670195  |\n",
      "| 16.12 |          ('breast', 'cancer')         |   1.3988305731303892  |\n",
      "|  27.7 |           ('cancer', 'risk')          |   1.380861302949147   |\n",
      "|  11.1 |           ('cancer', 'risk')          |   1.3485429201837915  |\n",
      "|  27.8 |           ('cancer', 'risk')          |   1.3423344143817892  |\n",
      "|  9.47 |           ('cancer', 'risk')          |   1.3173239312916332  |\n",
      "| 27.15 |           ('risk', 'women')           |   1.3165931987216708  |\n",
      "| 11.14 |          ('cancer', 'women')          |   1.2763834556641425  |\n",
      "|  9.50 |          ('cancer', 'women')          |   1.204806080463144   |\n",
      "| 24.15 |           ('cancer', 'risk')          |   1.0388051852766274  |\n",
      "| 24.20 |           ('cancer', 'risk')          |   1.0118169949091729  |\n",
      "| 17.28 |           ('risk', 'women')           |   1.0117361972095869  |\n",
      "| 11.47 |           ('risk', 'women')           |   0.848878487196564   |\n",
      "|  9.32 |           ('risk', 'women')           |   0.8440559239614074  |\n",
      "| 24.30 |          ('breast', 'cancer')         |   0.8409498957264315  |\n",
      "|  9.27 |           ('risk', 'women')           |   0.8281178861124261  |\n",
      "| 29.41 |           ('risk', 'women')           |   0.8216878150883121  |\n",
      "|  9.12 |           ('risk', 'women')           |   0.7961460603527165  |\n",
      "| 24.27 |           ('risk', 'women')           |   0.7568566576682306  |\n",
      "| 24.29 |          ('breast', 'cancer')         |   0.739764792047451   |\n",
      "| 24.37 |          ('breast', 'cancer')         |   0.6911886981755959  |\n",
      "| 29.24 |           ('risk', 'women')           |   0.6798903846720683  |\n",
      "|  25.2 |           ('cancer', 'risk')          |   0.5648644827848683  |\n",
      "| 11.43 |           ('risk', 'women')           |   0.5514460188494751  |\n",
      "| 11.25 |           ('risk', 'women')           |   0.5226426428547968  |\n",
      "| 16.31 |          ('breast', 'women')          |   0.5182444951927788  |\n",
      "|  9.30 |           ('risk', 'women')           |  0.49154094619524286  |\n",
      "| 11.48 |          ('cancer', 'women')          |  0.47914847273329697  |\n",
      "| 11.36 |          ('cancer', 'women')          |    0.47708927703255   |\n",
      "| 25.27 |           ('cancer', 'risk')          |   0.4565814746191844  |\n",
      "| 27.32 |           ('cancer', 'risk')          |   0.4555586448101976  |\n",
      "| 16.11 |          ('cancer', 'women')          |  0.44197563396079426  |\n",
      "| 11.12 |          ('cancer', 'women')          |   0.4217167535975052  |\n",
      "| 16.50 |           ('risk', 'women')           |  0.39440125073672944  |\n",
      "|  9.14 |           ('risk', 'women')           |   0.3904999577830127  |\n",
      "|  9.48 |          ('cancer', 'women')          |  0.38552684358286887  |\n",
      "| 24.10 |           ('cancer', 'risk')          |   0.3429163518862394  |\n",
      "|  3.46 |           ('risk', 'women')           |   0.3043221572031592  |\n",
      "|  9.3  |           ('risk', 'women')           |   0.2720241218661238  |\n",
      "|  9.24 |          ('cancer', 'women')          |  0.23418726749831215  |\n",
      "|  9.5  |           ('risk', 'women')           |  0.22933532299192966  |\n",
      "|  3.34 |           ('risk', 'women')           |  0.21990951724182783  |\n",
      "|  25.3 |          ('cancer', 'women')          |  0.21942721641521443  |\n",
      "|  9.17 |           ('risk', 'women')           |  0.20186834583176647  |\n",
      "| 11.11 |           ('risk', 'women')           |  0.19730332954835966  |\n",
      "|  6.46 |           ('risk', 'women')           |  0.18012251891360995  |\n",
      "| 11.23 |           ('risk', 'women')           |   0.1536347482353399  |\n",
      "|  14.3 |           ('risk', 'women')           |  0.15139919609033498  |\n",
      "| 24.44 |           ('cancer', 'risk')          |  0.14169354601809114  |\n",
      "| 16.47 |          ('breast', 'cancer')         |   0.1313010117397459  |\n",
      "|  7.24 |           ('risk', 'women')           |  0.12310426885712819  |\n",
      "|  5.30 |          ('cancer', 'women')          |  0.11516083212616621  |\n",
      "|  15.1 |           ('risk', 'women')           |  0.08492326921466252  |\n",
      "| 15.27 |          ('breast', 'cancer')         |   0.0842761219048372  |\n",
      "| 16.42 |           ('risk', 'women')           |  0.07588115950983554  |\n",
      "| 16.25 |           ('risk', 'women')           |  0.029541023950935177 |\n",
      "| 16.28 |           ('cancer', 'risk')          |  0.020482055070251463 |\n",
      "|  3.32 |           ('risk', 'women')           |  0.01776163860310158  |\n",
      "|  4.31 |           ('cancer', 'risk')          |  0.011380552252469601 |\n",
      "|  7.32 |           ('risk', 'women')           | -0.003146539926899606 |\n",
      "|  16.2 |           ('risk', 'women')           | -0.013694258902521884 |\n",
      "| 20.25 |           ('risk', 'women')           | -0.018708085330286686 |\n",
      "| 16.37 |           ('risk', 'women')           | -0.030475697278855303 |\n",
      "|  7.18 |           ('risk', 'women')           | -0.030712095239382987 |\n",
      "| 15.29 |           ('cancer', 'risk')          |  -0.04815822093151803 |\n",
      "|  4.34 |           ('risk', 'women')           | -0.048879733741196085 |\n",
      "| 10.37 |           ('risk', 'women')           |  -0.09609310049825237 |\n",
      "|  10.1 |           ('risk', 'women')           |  -0.11544898520116556 |\n",
      "|  4.45 |          ('cancer', 'women')          |  -0.11936727402148185 |\n",
      "+-------+---------------------------------------+-----------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['24.48',\n",
       " '24.49',\n",
       " '24.50',\n",
       " '24.23',\n",
       " '24.7',\n",
       " '9.37',\n",
       " '24.47',\n",
       " '11.35',\n",
       " '24.2',\n",
       " '24.3']"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = str(input('Select your preffered model: tfidf / bm25 / dfi'))\n",
    "print(mod)\n",
    "entry = str(input('Enter a query: '))\n",
    "results = Search(entry, mod)\n",
    "if isinstance(results, list):\n",
    "    printResults(results)\n",
    "res = []\n",
    "for i in range(10):\n",
    "    res.append(results[i][0])\n",
    "res\n",
    "\n",
    "# Global warming\n",
    "# tfidf -> 10.37, 30.36, 20.21 10.4 30.49 5.34 5.44 5.11 7.27 5.30 5.17 2.4\n",
    "# bm25 -> 30.36 30.49 20.21 10.37 5.34 10.4 5.44 5.11 7.27  5.30  5.17 2.4 \n",
    "# dfi -> '30.36','10.37','30.49','20.21','5.34','5.44','5.11','10.4','7.27','5.30'\n",
    "\n",
    "# champions league\n",
    "# tfidf -> '12.11','12.8',12.7','8.29','28.14','8.17','12.37','28.50','14.33','26.5'\n",
    "# bm25 -> '12.7','12.11','28.14','14.33','8.17','12.8','28.50','28.43','28.25','28.5'\n",
    "# dfi -> '12.7','12.11','28.14','12.8','14.33','8.17','28.43','28.50','28.25','12.37'\n",
    "\n",
    "# racism\n",
    "# tfidf -> '3.3','4.35','30.5','5.46','13.47','13.48','3.30','4.9','26.42','4.38'\n",
    "# bm25 -> '3.3','30.5','4.35','5.46','13.47','13.48','3.30','4.9','26.42','4.38'\n",
    "# dfi -> '3.3','30.5','4.35','5.46','13.47','13.48','3.30','4.9','26.42','4.38'\n",
    "\n",
    "# alcohol and drug abuse\n",
    "# tfidf -> '23.20','16.22','4.9','3.1','11.50','8.36','8.20','15.10','11.23','16.44'\n",
    "# bm25 -> '23.20','16.22','4.9','3.1','11.50','8.36','8.20','11.23','16.44','15.10'\n",
    "# dfi -> '23.20','16.22','4.9','11.50','3.1','8.36','8.20','11.23','3.28','15.10'\n",
    "\n",
    "# exercising for kids\n",
    "# tfidf -> '23.26','23.40','23.29','23.35','23.44','23.14','23.28','23.38','23.43','23.10'\n",
    "# bm25 -> '23.26','23.40','23.29','23.35','23.44','23.28','23.14','23.38','23.43','23.10'\n",
    "# dfi -> '23.40','23.26','23.29','23.35','23.28','23.44','23.14','23.38','23.10','23.43'\n",
    "\n",
    "# obama vs clinton election\n",
    "# tfidf -> '4.7','5.35','5.24','10.29','1.26','10.17','5.42','10.15','10.18','10.11'\n",
    "# bm25 -> '4.7','5.35','5.24','1.26','5.42','5.48','10.17','10.29','10.15','5.27'\n",
    "# dfi -> '4.7','5.24','5.35','1.26','5.42','5.48','10.17','10.43','5.27','10.18'\n",
    "\n",
    "# health insurance\n",
    "# tfidf -> '24.31','24.32','25.41','24.29','25.9','25.16','25.27','25.48','25.7','25.17'\n",
    "# bm25 -> '24.32','24.31','25.41','25.9','24.29','25.48','25.8','25.39','25.16','25.27'\n",
    "# dfi -> '24.32','24.31','24.29','25.41','25.9','25.8','25.48','25.27','25.7','24.26'\n",
    "\n",
    "# breast cancer risk for women\n",
    "# tfidf -> '24.48','24.49','24.50','24.47','24.23','24.6','24.7','9.37','24.2','11.35'\n",
    "# bm25 -> '24.48','24.49','24.50','24.23','24.47','11.35','24.7','24.6','9.37','24.3'\n",
    "# dfi -> '24.48','24.49','24.50','24.23','24.7','9.37','24.47','11.35','24.2','24.3'\n",
    "\n",
    "# cancer risk for smokers\n",
    "# tfidf -> '24.7','27.35','24.34','24.3','24.48','24.49','24.50','24.2','27.19','9.50'\n",
    "# bm25 -> '24.3','24.7','27.35','24.34','29.40','24.39','11.35','24.6','24.48','24.49'\n",
    "# dfi -> '24.3','24.7','27.35','29.40','24.34','24.39','11.35','24.48','24.49','24.50'\n",
    "\n",
    "# alzheimer risk\n",
    "# tfidf -> '27.28','11.47','9.30','11.29','11.50','27.47','9.27','11.17','11.25','9.33'\n",
    "# bm25 -> '27.28','9.30','11.47','11.29','11.17','9.27','11.50','27.47','9.33','11.25'\n",
    "# dfi -> '27.28','11.47','9.30','11.29','11.17','11.50','9.27','27.47','9.33','11.25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf: 0.7776666666666666 \n",
      "okapiBM25: 0.7712380952380953 \n",
      "DFI: 0.7813333333333334\n"
     ]
    }
   ],
   "source": [
    "okapiQueries = defaultdict(list)\n",
    "tfidfQueries = defaultdict(list)\n",
    "dfiQueries = defaultdict(list)\n",
    "\n",
    "tfidfQueries['global warming'] = [1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "okapiQueries['global warming'] = [1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "dfiQueries['global warming'] =   [1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "tfidfQueries['champions league'] = [1, 1, 1, 1, 1, 1, 1, 0, 0, 1]\n",
    "okapiQueries['champions league'] = [1, 1, 1, 0, 1, 1, 0, 0, 0, 0]\n",
    "dfiQueries['champions league'] =   [1, 1, 1, 1, 0, 1, 0, 0, 0, 1]\n",
    "\n",
    "tfidfQueries['racism'] = [1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
    "okapiQueries['racism'] = [1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
    "dfiQueries['racism'] =   [1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "tfidfQueries['alcohol and drug abuse'] = [1, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "okapiQueries['alcohol and drug abuse'] = [1, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "dfiQueries['alcohol and drug abuse'] =   [1, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "tfidfQueries['exercising for kids'] = [1, 1, 1, 1, 1, 1, 0, 1, 1, 0]\n",
    "okapiQueries['exercising for kids'] = [1, 1, 1, 1, 1, 0, 1, 1, 1, 0]\n",
    "dfiQueries['exercising for kids'] =   [1, 1, 1, 1, 0, 1, 1, 1, 0, 1]\n",
    "\n",
    "tfidfQueries['obama vs clinton election'] = [1, 1, 0, 1, 1, 0, 1, 0, 0, 0]\n",
    "okapiQueries['obama vs clinton election'] = [1, 1, 1, 1, 1, 1, 0, 1, 0, 1]\n",
    "dfiQueries['obama vs clinton election'] =   [1, 1, 1, 1, 1, 1, 0, 1, 1, 0]\n",
    "\n",
    "tfidfQueries['health insurance'] = [1, 1, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "okapiQueries['health insurance'] = [1, 1, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "dfiQueries['health insurance'] =   [1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "tfidfQueries['breast cancer risk for women'] = [1, 1, 1, 1, 1, 1, 0, 1, 0, 0]\n",
    "okapiQueries['breast cancer risk for women'] = [1, 1, 1, 1, 1, 0, 0, 1, 0, 0]\n",
    "dfiQueries['breast cancer risk for women'] =   [1, 1, 1, 1, 0, 1, 1, 0, 0, 0]\n",
    "\n",
    "tfidfQueries['cancer risk for smokers'] = [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "okapiQueries['cancer risk for smokers'] = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "dfiQueries['cancer risk for smokers'] =   [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "\n",
    "tfidfQueries['alzheimer risk'] = [1, 1, 1, 1, 1, 0, 1, 0, 0, 0]\n",
    "okapiQueries['alzheimer risk'] = [1, 1, 1, 1, 0, 1, 1, 0, 0, 0]\n",
    "dfiQueries['alzheimer risk'] =   [1, 1, 1, 1, 0, 1, 1, 0, 0, 0]\n",
    "\n",
    "\n",
    "def MAPscore(hmap, n):\n",
    "    score = []\n",
    "    for k in hmap.keys():\n",
    "        sums = 0\n",
    "        a=0\n",
    "        for i, v in enumerate(hmap[k]):\n",
    "            sums += v\n",
    "            a+=sums/(i+1)\n",
    "        score.append(a/n)\n",
    "    return sum(score)/n\n",
    "    \n",
    "print('tfidf: {} \\nokapiBM25: {} \\nDFI: {}'.format(MAPscore(tfidfQueries, 10),\n",
    "                                                   MAPscore(okapiQueries, 10),\n",
    "                                                   MAPscore(dfiQueries, 10)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CENG3530_IR_HW_Beyza_Kurt.ipynb",
   "provenance": [
    {
     "file_id": "14UphnBJ0rGcqrzsuhrVkS5djLUmFBViM",
     "timestamp": 1588838160752
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
